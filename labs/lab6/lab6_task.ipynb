{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0PNevS86zMh"
      },
      "source": [
        "# **COMP 2211 - Exploring Artificial Intelligence**  \n",
        "## Lab 6 · Multilayer Perceptron (MLP)\n",
        "\n",
        "Welcome to Lab 6!  \n",
        "Your mission is to design, train and evaluate a **Multilayer Perceptron (MLP)** that predicts a student's `final_grade` (`G3` in original dataset) using the altered version of [UCI Student-Performance dataset](https://archive.ics.uci.edu/ml/datasets/Student+Performance)\n",
        "\n",
        "![MLP Illustration](https://media.geeksforgeeks.org/wp-content/uploads/nodeNeural.jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xULOflOcmaZ"
      },
      "source": [
        "## **Tasks Breakdown**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRvz2pYK7Pa2"
      },
      "source": [
        "1. **Task0 · Import & visualise** the dataset (feature histograms + correlation heat-map).  \n",
        "2. **Task1 · Pre-process** by standardising every feature (Z-score) using training-set statistics.  \n",
        "3. **Task2 · Build** a compact MLP (≤ 20 000 params) with Keras `Dense`, `BatchNorm` and `Dropout` layers.  \n",
        "4. **Task3 · Compile & train** the model (Adam optimizer + MSE loss, monitor MAE & MSE validation scores), save the best model as **`lab6_model.keras`**.  \n",
        "5. **Task4 · Evaluate & save**: compute MSE, RMSE & R², draw *Actual vs Predicted* scatter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxY2MkCn7pe9"
      },
      "source": [
        "## **Task 0 · import & visualise the dataset** (no todos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVL5RLjuIucF"
      },
      "source": [
        "The dataset, collected by Paulo Cortez *et al.* and hosted on the UCI Machine Learning Repository, combines demographic, social, and academic information for Portuguese secondary-school students. Here are the features:\n",
        "\n",
        "\n",
        "#### Feature list\n",
        "\n",
        "| Formal name | Description |\n",
        "|-------------|-------------|\n",
        "| `school` | Secondary school attended |\n",
        "| `gender` | Student's gender |\n",
        "| `age` | Age in years (15 - 22) |\n",
        "| `address` | Home setting |\n",
        "| `family_size` | Size of household (number of people living together) |\n",
        "| `parent_status` | Whether parents live together |\n",
        "| `mother_education` | Mother's highest education level (0 = none … 4 = tertiary) |\n",
        "| `father_education` | Father's highest education level |\n",
        "| `mother_job` | Mother's main occupation |\n",
        "| `father_job` | Father's main occupation |\n",
        "| `school_reason` | Main reason for choosing the current school |\n",
        "| `guardian` | Primary legal guardian |\n",
        "| `travel_time` | One-way commute time to school (1 = < 15 min … 4 = > 60 min) |\n",
        "| `study_time` | Weekly hours of individual study (1 = < 2 h … 4 = ≥ 10 h) |\n",
        "| `failures` | Number of past class failures (0 - 3) |\n",
        "| `school_support` | Extra academic support provided by the school |\n",
        "| `family_support` | Educational support provided by family |\n",
        "| `extra_paid_classes` | Attendance at paid private classes |\n",
        "| `extracurricular_activities` | Participation in sports, arts, clubs, … |\n",
        "| `nursery_attended` | Attendance at nursery school |\n",
        "| `higher_education` | Intention to pursue higher education |\n",
        "| `internet_access` | Internet access at home |\n",
        "| `romantic_relationship` | Ongoing romantic relationship |\n",
        "| `family_relationship` | Quality of family relationships (1 = very bad … 5 = excellent) |\n",
        "| `free_time` | Free time after school (1 = very low … 5 = very high) |\n",
        "| `going_out` | Evenings out with friends (same 1-5 scale) |\n",
        "| `weekday_alcohol_consumption` | Alcohol intake Monday-Friday (1 = none … 5 = very high) |\n",
        "| `weekend_alcohol_consumption` | Alcohol intake on weekends |\n",
        "| `health_status` | Current health condition (1 = very poor … 5 = very good) |\n",
        "| `absences` | Number of school absences |\n",
        "| `first_period_grade` | Grade at the end of the 1st term (0 - 20) |\n",
        "| `second_period_grade` | Grade at the end of the 2nd term (0 - 20) |\n",
        "\n",
        "\n",
        "\n",
        "#### Categorical Feature Encoded List:\n",
        "\n",
        "| Feature                      | Integer → Meaning                                                                 |\n",
        "|------------------------------|-----------------------------------------------------------------------------------|\n",
        "| **school**                   | 0 = GP, 1 = MS                                                                    |\n",
        "| **gender**                   | 0 = F, 1 = M                                                                      |\n",
        "| **address**                  | 0 = R, 1 = U                                                                      |\n",
        "| **family_size**              | 0 = LE3, 1 = GT3                                                                  |\n",
        "| **parent_status**            | 0 = T, 1 = A                                                                      |\n",
        "| **mother_job**               | 0 = services, 1 = at_home, 2 = teacher, 3 = health, 4 = other                     |\n",
        "| **father_job**               | 0 = at_home, 1 = teacher, 2 = other, 3 = services, 4 = health                     |\n",
        "| **school_reason**            | 0 = reputation, 1 = other, 2 = home, 3 = course                                   |\n",
        "| **guardian**                 | 0 = father, 1 = other, 2 = mother                                                 |\n",
        "| **school_support**           | 0 = yes, 1 = no                                                                   |\n",
        "| **family_support**           | 0 = yes, 1 = no                                                                   |\n",
        "| **extra_paid_classes**       | 0 = no, 1 = yes                                                                   |\n",
        "| **extracurricular_activities** | 0 = no, 1 = yes                                                                 |\n",
        "| **nursery_attended**         | 0 = no, 1 = yes                                                                   |\n",
        "| **higher_education**         | 0 = no, 1 = yes                                                                   |\n",
        "| **internet_access**          | 0 = yes, 1 = no                                                                   |\n",
        "| **romantic_relationship**    | 0 = no, 1 = yes\n",
        "\n",
        "Attribute to predict: `final_grade`, which is the grade at the end of the 3rd term."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_OpDo1T04f0"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  import matplotlib.pyplot as plt\n",
        "  import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFrFYOap5pV1"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    X_train = pd.read_csv(\"X_train.csv\")\n",
        "    X_test  = pd.read_csv(\"X_test.csv\")\n",
        "    y_train = pd.read_csv(\"y_train.csv\").squeeze()\n",
        "    y_test  = pd.read_csv(\"y_test.csv\").squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RhaZihH48XM0",
        "outputId": "f50eca18-8abe-4d4c-ee03-74d93dcfe7d1"
      },
      "outputs": [],
      "source": [
        "#  Visualise attribute, distributions & correlations\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_df = X_train.copy()\n",
        "    train_df[y_train.name] = y_train\n",
        "    display(train_df.head())\n",
        "\n",
        "    # Histograms\n",
        "    ax = train_df.hist(bins=40, figsize=(14,10))\n",
        "    plt.suptitle(\"Feature distributions (training set)\", y=1.03, fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
        "    plt.show()\n",
        "\n",
        "    # Correlation heat‑map\n",
        "    plt.figure(figsize=(20,16))\n",
        "    sns.heatmap(train_df.corr(), annot=True, fmt=\".1f\", cmap=\"coolwarm\")\n",
        "    plt.title(\"Correlation heat‑map\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu39zHRx-Frh"
      },
      "source": [
        "## **Task 1 · Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44cpOhCq-IUV"
      },
      "source": [
        "### Z-score Standardisation\n",
        "\n",
        "All features are numeric, so we apply **Z-score normalisation** using **training set** mean and variance:\n",
        "\n",
        "$$\n",
        "z \\;=\\;\\frac{x - \\mu_{\\text{train}}}{\\sigma_{\\text{train}}}\n",
        "$$\n",
        "\n",
        "**Important Notes**\n",
        "\n",
        "- Statistics ($\\mu_{\\text{train}}, \\sigma_{\\text{train}}$) are computed **only** on the **training** set, as we always assume we haven't obtain info of the test set when train the model.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "q9oVe_JWqLHl"
      },
      "outputs": [],
      "source": [
        "def preprocess(X_train,X_test,y_train,y_test):\n",
        "    \"\"\"\n",
        "    Perform z-score normalisation on feature and target data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_train : pandas.DataFrame\n",
        "        Feature matrix for the training split.\n",
        "    X_test : pandas.DataFrame\n",
        "        Feature matrix for the test split.\n",
        "    y_train : pandas.Series\n",
        "        Target vector for the training split.\n",
        "    y_test : pandas.Series\n",
        "        Target vector for the test split.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple\n",
        "        (\n",
        "        X_train_std : pandas.DataFrame, standardised training features\n",
        "        X_test_std  : pandas.DataFrame, standardised test features\n",
        "        y_train_std : pandas.Series, standardised training targets\n",
        "        y_test_sted  : pandas.Series, standardised test targets\n",
        "        )\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ###############################################################################\n",
        "    # TODO: your code starts here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # TODO: your code ends here\n",
        "    ###############################################################################\n",
        "\n",
        "    return (\n",
        "        X_train_std,\n",
        "        X_test_std,\n",
        "        y_train_std,\n",
        "        y_test_std\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Srh3cjy3poM_",
        "outputId": "24ca0d85-adc6-4880-d206-4eef3323053f"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  X_train = pd.read_csv(\"X_train.csv\")\n",
        "  X_test  = pd.read_csv(\"X_test.csv\")\n",
        "  y_train = pd.read_csv(\"y_train.csv\").squeeze()\n",
        "  y_test  = pd.read_csv(\"y_test.csv\").squeeze()\n",
        "  print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubgfwFOORpu7"
      },
      "source": [
        "## **Task 2 · Model Building**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdoYm2cCRzEB"
      },
      "source": [
        "We'll build a **compact multilayer perceptron** that predicts the `final_grade` (Float value ranged from 0 - 20) from student performance features.  \n",
        "Your model can *NOT* contain more than **20,000 parameters**.\n",
        "\n",
        "#### Common layers used in MLP\n",
        "\n",
        "| # | Layer | What it does | Why it helps here |\n",
        "|---|-------|--------------|-------------------|\n",
        "| **1** | **Dense**<br>([`keras.layers.Dense`](https://keras.io/api/layers/core_layers/dense/)) | Fully-connected computation  \\(y = W · x + b\\) followed by **ReLU**. | Captures non-linear relationships between socio-demographic features and exam marks. |\n",
        "| **2** | **Batch Normalization**<br>([`keras.layers.BatchNormalization`](https://keras.io/api/layers/normalization_layers/batch_normalization/)) | Normalises layer activations to zero-mean/ unit-variance during training. | Speeds convergence and allows slightly higher learning rates on small tabular sets. |\n",
        "| **3** | **Dropout**<br>([`keras.layers.Dropout`](https://keras.io/api/layers/regularization_layers/dropout/)) | Randomly “switches off” neurons each step. | Mitigates over-fitting on the modest-sized UCI dataset. |\n",
        "\n",
        "> **Regularisation note** - Every Dense layer also uses **l2 weight-decay** (`kernel_regularizer=keras.regularizers.l2(1e-4)`), adding more generalisation power without adding parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gC3ZMH-XrIJp"
      },
      "outputs": [],
      "source": [
        "def create_model(input_dim):\n",
        "    \"\"\"\n",
        "    Tiny fully-connected network for regression.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_dim : int\n",
        "        Number of input features (after preprocessing).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    keras.Sequential\n",
        "        The MLP model.\n",
        "    \"\"\"\n",
        "    l2 = regularizers.l2(1e-4)\n",
        "\n",
        "    ###############################################################################\n",
        "    # TODO: your code starts here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # TODO: your code ends here\n",
        "    ###############################################################################\n",
        "\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "ql4J3nN5zV-n",
        "outputId": "9dc78e37-73ee-4dc8-bd32-163723c198cf"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  # Your model summary. Make sure it doesn't contain more than 20,000 Total params.\n",
        "  warnings.filterwarnings('ignore',message=r'.*input_shape.*Sequential.*',category=UserWarning)\n",
        "  X_train_std, X_test_std, y_train_std, y_test_std = preprocess(\n",
        "    X_train, X_test, y_train, y_test\n",
        "    )\n",
        "  model = create_model(X_train_std.shape[1])\n",
        "  model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwA6nLYeTn_3"
      },
      "source": [
        "## **Task 3 · Compile & Train**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGHjHS8vWE2L"
      },
      "source": [
        "For detailed information on configuring your model before training, including all available parameters for the `.compile()` method, see the [TensorFlow Keras API documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile):\n",
        "\n",
        "- **Model.compile** configures the model for training by specifying:\n",
        "  - `optimizer` (e.g. `Adam`, `RMSprop`, `SGD`, …). You need to use **Adam** in this task.\n",
        "  - `loss` (e.g. `\"mse\"`, `MeanSquaredError()`, …). You need to use **MSE** in this task.\n",
        "  - `metrics` (e.g. `[\"mae\", \"mse\"]`, `BinaryAccuracy()`, …). You need to use **MAE and MSE** in this task.\n",
        "  \n",
        "For a hands-on example showing how to set up the training workflow—including `.compile()`, `.fit()`, and monitoring validation metrics—see the [Keras guide](https://www.tensorflow.org/guide/keras/training_with_built_in_methods#the_compile_method_specifying_a_loss_metrics_and_an_optimizer) on built-in training methods:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4dHkZnQD-Ce"
      },
      "source": [
        "### Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cEXTK5emZnq7"
      },
      "outputs": [],
      "source": [
        "def compile_model(model):\n",
        "  \"\"\"\n",
        "    Compile a Keras model for a regression task.\n",
        "  \"\"\"\n",
        "  ###############################################################################\n",
        "  # TODO: your code starts here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # TODO: your code ends here\n",
        "  ###############################################################################\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwMhyRcHVwxS"
      },
      "source": [
        "> ### **Practical Tip: Diagnosing Over-fitting and Under-fitting & How Hyperparameters Affect Training**  \n",
        ">\n",
        "> When you train a model, you'll monitor two curves over epochs: **training loss** (and metrics) vs. **validation loss** (and metrics). Their shapes tell you if your model is learning well, under-fitting, or over-fitting. Here's how to adjust common hyperparameters and what you'll observe in those curves:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwjQOstuWgDx"
      },
      "source": [
        "> #### 1. Epochs  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RniBZtorWnfJ"
      },
      "source": [
        " - **Too few** (under-fitting):  \n",
        "   - *What you'll see:* Both training & validation loss stay high and flat.  \n",
        "   - *Fix:* Increase epochs so the model has more opportunity to learn.  \n",
        " - **Too many** (over-fitting):  \n",
        "   - *What you'll see:* Training loss continues to drop, but validation loss bottoms out then rises.  \n",
        "   - *Fix:* Lower the number of epochs or add regularization (e.g. Dropout, weight decay)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfLe_8xJWxwR"
      },
      "source": [
        "> #### 2. Batch Size  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXny9W6iW1PT"
      },
      "source": [
        " - **Smaller** (e.g. 16 → 8):  \n",
        "   - *Effect:* Noisier gradient estimates → more “wiggle” in the loss curves. Can help escape local minima but may slow convergence.  \n",
        "   - *What you'll see:* Loss curves jump up-and-down but might find a better final minimum.  \n",
        " - **Larger** (e.g. 64 → 128):  \n",
        "   - *Effect:* Smoother, more stable training but potentially gets stuck in sharp minima and over-fits.  \n",
        "   - *What you'll see:* Very smooth, monotonic decrease in training loss; if validation loss rises, you may be over-fitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRZC9x7-W6xr"
      },
      "source": [
        "> #### 3. Learning Rate (via Optimizer)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_DiaLI9W9SP"
      },
      "source": [
        " - **Higher LR**:  \n",
        "   - *Effect:* Bigger steps → faster initial decrease but risk of divergence or oscillation.  \n",
        "   - *What you'll see:* Loss may bounce up and down wildly or even increase.  \n",
        " - **Lower LR**:  \n",
        "   - *Effect:* Smaller steps → more stable but slower convergence; may get stuck if too low.  \n",
        "   - *What you'll see:* Gradual, steady decline; risk of plateauing early."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8FscRnlXAs6"
      },
      "source": [
        "> #### 4. Regularization  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ppb2Oj9MXCKX"
      },
      "source": [
        " - **Dropout**:  \n",
        "   - *Effect:* Randomly “drops” neurons each batch to prevent co-adaptation.  \n",
        "   - *What you'll see:* Training loss will be higher (noisier), but validation loss should flatten rather than rise steeply if over'fitting was a problem.  \n",
        " - **Weight Decay (L2 Regularization)**:  \n",
        "   - *Effect:* Penalizes large weights → smoother function.  \n",
        "   - *What you'll see:* Similar to Dropout: less gap between training & validation loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4VfHg_tXFwd"
      },
      "source": [
        "> #### 5. Model Capacity  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDxSJ8WoXI_L"
      },
      "source": [
        " - **Too small** (under-fitting):  \n",
        "   - *Fix:* Add more layers or neurons.  \n",
        "   - *What you'll see:* Both curves high, little improvement across epochs.  \n",
        " - **Too large** (over'fitting):  \n",
        "   - *Fix:* Reduce layers or neurons; add regularization.  \n",
        "   - *What you'll see:* Training loss very low, validation loss rising after a point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-LDKz_bXJ9U"
      },
      "source": [
        "> #### 6. Putting It All Together  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xe17m0mWPlh"
      },
      "source": [
        " 1. **Plot** `history.history['loss']` vs. `history.history['val_loss']`.  \n",
        " 2. **Assess**:  \n",
        "    - **Under-fit** (flat & high): increase capacity, epochs, or LR.  \n",
        "    - **Over-fit** (train ↓, val ↑): add Dropout/L2, reduce capacity or epochs, increase batch size.  \n",
        "    - **Plateau** (both flat): adjust learning rate or try a different optimizer.  \n",
        " 3. **Iterate**: Change one hyperparameter at a time to see its specific effect.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaQRSnjiD4r2"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCnOpomjr5IW",
        "outputId": "4bb54b87-ccea-4b94-d7d3-e8d4580534a5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "\n",
        "    X_train_std, X_test_std, y_train_std, y_test_std = preprocess(\n",
        "    X_train, X_test, y_train, y_test\n",
        "    )\n",
        "\n",
        "    model = compile_model(create_model(input_dim=X_train_std.shape[1]))\n",
        "\n",
        "    # Save only the best weights\n",
        "    checkpoint_cb = ModelCheckpoint(\n",
        "        filepath=\"best_weights.weights.h5\",\n",
        "        monitor=\"val_loss\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "        mode=\"min\",\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    ################################################################\n",
        "    # TODO: You can feel free to adjust the batch size, epochs... to achieve better results\n",
        "    # As long as don't make it too large as computation power is limited on Colab.\n",
        "    # And remember your model should have <= 10k parameters\n",
        "\n",
        "   \n",
        "\n",
        "    # TODO: your code ends here\n",
        "    ###############################################################################\n",
        "\n",
        "    # Load the best weights & Save the model\n",
        "    model.load_weights(\"best_weights.weights.h5\")\n",
        "    model.save(\"lab6_model.keras\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "id": "wC8BHfXenB-G",
        "outputId": "676656d6-f240-47db-d896-3e8aa252b10d"
      },
      "outputs": [],
      "source": [
        "# Visualize the training process\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    hist   = pd.DataFrame(history.history)\n",
        "    epochs = hist.index + 1\n",
        "\n",
        "    for key, ylabel in [(\"loss\", \"Mean-Squared Error\"),\n",
        "                        (\"mae\",  \"Mean Absolute Error\")]:\n",
        "        plt.figure(figsize=(8,4))\n",
        "        plt.plot(epochs, hist[key],        label=\"Train\")\n",
        "        plt.plot(epochs, hist[f\"val_{key}\"],label=\"Val\")\n",
        "        if key == \"loss\": plt.yscale(\"log\")\n",
        "        plt.xlabel(\"Epoch\"); plt.ylabel(ylabel)\n",
        "        plt.legend(); plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjlteV5eXo2R"
      },
      "source": [
        "## **Task 4 · Evaluate & Save**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLoz0_Cfzo2s"
      },
      "source": [
        "### **Evaluation & Visualisation**\n",
        "\n",
        "* **MSE (Mean Squared Error)**  \n",
        "  Measures the average of the squared differences between predictions and true values, heavily penalizing larger errors to focus the model on big mistakes.  \n",
        "  $$ \\mathrm{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
        "\n",
        "* **RMSE (Root Mean Squared Error)**  \n",
        "  The square root of MSE, bringing the error back to the same units as the target for more intuitive interpretation while still emphasizing larger deviations.  \n",
        "  $$ \\mathrm{RMSE} = \\sqrt{\\mathrm{MSE}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} $$\n",
        "\n",
        "* **MAE (Mean Absolute Error)**  \n",
        "  Computes the average absolute difference between predictions and true values, offering a straightforward “per‐unit” error metric that’s robust to outliers.  \n",
        "  $$ \\mathrm{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} \\lvert y_i - \\hat{y}_i \\rvert $$\n",
        "\n",
        "* **R² (Coefficient of Determination)**  \n",
        "  Reflects the proportion of variance in the true values that the model explains (1 = perfect fit, values <0 indicate worse than guessing the mean).  \n",
        "  $$ R^2 = 1 - \\frac{\\displaystyle\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\displaystyle\\sum_{i=1}^{n} (y_i - \\bar{y})^2} $$\n",
        "\n",
        "Let's compute these on the **standardised** test split and plot predictions.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "RyjK8AdYvuvJ",
        "outputId": "38518c6b-a600-48ff-d1dd-5a6eb2ca5cb9"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "      model = keras.models.load_model(\"lab6_model.keras\")\n",
        "      y_pred_std = model.predict(X_test_std, verbose=0).flatten()\n",
        "\n",
        "      y_mean = y_train.mean()\n",
        "      y_std  = y_train.std(ddof=0)\n",
        "      y_true = y_test_std * y_std + y_mean\n",
        "      y_pred = y_pred_std * y_std + y_mean\n",
        "\n",
        "      mse  = mean_squared_error(y_true, y_pred)\n",
        "      rmse = np.sqrt(mse)\n",
        "      mae  = mean_absolute_error(y_true, y_pred)\n",
        "      r2   = r2_score(y_true, y_pred)\n",
        "      print(f\"MSE={mse:.4f}  RMSE={rmse:.4f}  MAE={mae:.4f}  R²={r2:.4f}\")\n",
        "\n",
        "      min_val = min(y_true.min(), y_pred.min())\n",
        "      max_val = max(y_true.max(), y_pred.max())\n",
        "\n",
        "      plt.figure(figsize=(6, 6))\n",
        "      plt.scatter(y_true, y_pred, s=20, alpha=0.55)\n",
        "      plt.plot([min_val, max_val],\n",
        "         [min_val, max_val],\n",
        "         color='purple',\n",
        "          linestyle=':',\n",
        "         linewidth=1.5,\n",
        "         alpha=0.75,\n",
        "         label='Ideal: $y = x$')\n",
        "      plt.xlabel(\"Actual final_grade\")\n",
        "      plt.ylabel(\"Predicted final_grade\")\n",
        "      plt.title(f\"Actual vs Predicted  (R²={r2:.3f})\")\n",
        "      plt.tight_layout()\n",
        "      plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmTuMuYd37mx"
      },
      "source": [
        "## **Grading Scheme**\n",
        "\n",
        "Please export your notebook on Colab as `lab6_tasks.py` (File -> Download -> Download .py), download your `lab6_model.keras` model weight file, compress them into `lab6_tasks.zip` and submit.\n",
        "\n",
        "\n",
        "* You get **2 points** for data preprocessing (task 1)\n",
        "* You get **2 points** for the valid implementation of the MLP model, with model parameter <= 20,000 (task 2)\n",
        "* You get **2 points** for model compilation (task 3)\n",
        "* You get **2 points** for achieving an RMSE score of <= 2.4 on given testcase\n",
        "* You get **2 points** for achieving an RMSE score of <= 2.4 on hidden testcase\n",
        "\n",
        "**IMPORTANT**: Note that if you have a large model with > 20,000 parameter, Your testcase 4 and 5 will automatically **get 0 point** because of limit computation power for ZINC."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
