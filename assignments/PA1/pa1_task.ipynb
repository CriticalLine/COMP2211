{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNPqZDGjOR3w"
      },
      "source": [
        "## COMP2211 PA1: Text Sentiment Analysis with Naïve Bayes\n",
        "Welcome to PA1! In this assignment, we will build a **Naïve Bayes Model** to analyze user reviews about Video Games from Amazon. This task is the base for content moderation, user engagement analysis, and surfacing high-quality content in industry.\n",
        "\n",
        "Although we will not cover **Natural Language Processing (NLP)** in COMP2211, this assignment provides a practical introduction to text analysis, and it's fun to have a taste of it :). Rest assured, we only ask you to implement techniques you've learned in the course.\n",
        "\n",
        "Your system will perform two key analytical tasks:\n",
        "1.  **Sentiment Classification**: Build a **Naïve Bayes Classifier** to determine whether reviews are **positive** or **negative**.\n",
        "2.  **Feature Analysis**: Identify the most important words driving sentiment predictions through log-odds analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9irbJ7AC6OJG"
      },
      "source": [
        "#### Part 0: Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyovfhUtYjFi"
      },
      "source": [
        "##### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UYuNuCn6vxP"
      },
      "outputs": [],
      "source": [
        "#import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHvNadWoS_vM"
      },
      "source": [
        "##### Check the uploaded data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhLrcBK-Hhr-"
      },
      "source": [
        "If you have successfully uploaded the dataset, you should see \"dataset.csv\" in the output from the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4jRGSLvHdtL",
        "outputId": "43d21244-74f9-4b12-f4db-9ad1f63543f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "private.csv  public.csv  \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gm6m2io8LNT"
      },
      "source": [
        "##### Import dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo4a1YKQ7bCm",
        "outputId": "01b70e2f-ad60-488d-e168-49474d48711a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   rating                   title  \\\n",
            "0       1  don't waste your money   \n",
            "1       1        Limited run fail   \n",
            "2       4               It coming   \n",
            "3       5           A joy to play   \n",
            "4       5                  awsome   \n",
            "\n",
            "                                                text  \n",
            "0                                  not worth a penny  \n",
            "1  the game itself is wonderful. limited run ruin...  \n",
            "2  March 15 this character will be released at st...  \n",
            "3  I usually suck at games like this (first time ...  \n",
            "4  my grandson covered his xbo and remote in the ...  \n"
          ]
        }
      ],
      "source": [
        "#Load the public dataset, convert them from pandas dataframe to numpy array\n",
        "if __name__ == \"__main__\":\n",
        "  df_data = pd.read_csv(\"dataset.csv\")\n",
        "  print(df_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkqHVc298Y0d"
      },
      "source": [
        "As you can see in the output, the dataset includes 3 columns:\n",
        "1. **Rating**: A float number which could be **1.0, 2.0, 3.0, 4.0, 5.0**. The higher the number, the better the reviewer thinks the game is. However, to ensure that the data has a relatively obvious emotional tendency, we delete those reviews with rating equals to **3.0**. Later in this assignment, we will assign **\"positive\"** labels to reviews with a rating of 4.0 or 5.0, and **\"negative\"** labels to the other.\n",
        "2. **Title** of the review\n",
        "3. **Text**: Content of the review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JauEAb_Smmf"
      },
      "source": [
        "##### Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Si0S20ES6cq"
      },
      "source": [
        "To ensure our model can process the data efficiently, we need to **preprocess the text** with the following steps:\n",
        "\n",
        "- Convert all text data to **Strings**.\n",
        "- Normalize text to **lowercase** to prevent **case sensitivity issues**. e.g., \"COMP\" vs \"comp\" would otherwise be treated as different words, which may unnecessarily increase computational complexity and degrade the classification accuracy.\n",
        "- **Remove punctuation, numbers, and special characters** from the text, so that the model can exclusively on meaningful words for prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbPIkd73getb"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # turn all the data into string\n",
        "    df_data['title'] = df_data['title'].astype(str)\n",
        "    df_data['text'] = df_data['text'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYXdSZresQ3S",
        "outputId": "0d5e7155-b763-4763-8670-f7707dda4cc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   rating                  title  \\\n",
            "0       1  dont waste your money   \n",
            "1       1       limited run fail   \n",
            "2       4              it coming   \n",
            "3       5          a joy to play   \n",
            "4       5                 awsome   \n",
            "\n",
            "                                                text  \n",
            "0                                  not worth a penny  \n",
            "1  the game itself is wonderful limited run ruine...  \n",
            "2  march  this character will be released at stan...  \n",
            "3  i usually suck at games like this first time p...  \n",
            "4  my grandson covered his xbo and remote in the ...  \n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  # 1. turn into lower cases\n",
        "  df_data['title'] = df_data['title'].str.lower()\n",
        "  df_data['text'] = df_data['text'].str.lower()\n",
        "\n",
        "  # 2. Remove punctuation, numbers, and special characters from the text\n",
        "  df_data['title'] = df_data['title'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
        "  df_data['title'] = df_data['title'].str.replace(r'\\d+', '', regex=True)\n",
        "  df_data['text'] = df_data['text'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
        "  df_data['text'] = df_data['text'].str.replace(r'\\d+', '', regex=True)\n",
        "\n",
        "  print(df_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtE8qmJ_cmE4"
      },
      "source": [
        "##### **Task0.1: Concatenate the title and text with weights**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEY5VG3ac0dB"
      },
      "source": [
        "For simplicity, we need to concatenate the title and text together, and analysis these \"full reviews\". However, usually the **title** concludes the main idea of a review, **so it should be considered more important** than the text when we try to predict its sentiment. If we just simply add them together, we may fail to represent the importance of title as they will be treated equal.\n",
        "\n",
        "So how to represent this importance?\n",
        "\n",
        "We try to use a simple and intuitive idea:\n",
        "**Repeat the title twice** before the text to double its weight in the combined review.\n",
        "\n",
        "This ensures words in the title appear with twice the frequency of words in the text, making them more prominent in upcoming analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYruawKScitu",
        "outputId": "a670f739-8d21-4759-d99e-ecbd9c02c73c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------Before Concatenating----------\n",
            "title    dont waste your money\n",
            "text         not worth a penny\n",
            "Name: 0, dtype: object\n",
            "----------After Concatenating----------\n",
            "dont waste your money dont waste your money not worth a penny\n"
          ]
        }
      ],
      "source": [
        "###Task0.1: Concatenate the title and text with weights###\n",
        "def concatenate_title_and_text(df_data):\n",
        "  '''\n",
        "  Args: A pandas DataFrame with 4 columns representing rating, title and text\n",
        "  Return: A pandas DataFrame with ONLY ONE columns representing the full review\n",
        "      including title repeated twice and text\n",
        "\n",
        "  You can refer to this webpage for your answer\n",
        "  https://www.geeksforgeeks.org/pandas/pandas-combine-columns/\n",
        "\n",
        "  IMPORTANT: To ensure your answer can be graded properly,\n",
        "        please make sure that your title comes before the text,\n",
        "        and keep ONE and ONLY ONE space between each words when catenating.\n",
        "  '''\n",
        "  ###TODO-1###\n",
        "  \n",
        "  \n",
        "  \n",
        "  ############\n",
        "  return df_result\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  full_review = concatenate_title_and_text(df_data)\n",
        "  print(\"----------Before Concatenating----------\")\n",
        "  print(df_data[['title', 'text']].iloc[0])\n",
        "  print(\"----------After Concatenating----------\")\n",
        "  print(full_review.iloc[0])\n",
        "  \n",
        "  # Expected Output:\n",
        "  # ----------Before Concatenating----------\n",
        "  # title    dont waste your money\n",
        "  # text         not worth a penny\n",
        "  # Name: 0, dtype: object\n",
        "  # ----------After Concatenating----------\n",
        "  # dont waste your money dont waste your money not worth a penny"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX3iFf8SmtPn"
      },
      "source": [
        "##### Vectorization for the words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJaTSn1oozRf"
      },
      "source": [
        "In this section, we vectorize the review data into a matrix with shape $(d, V)$, where d is the number of reviews and V is the number of words (size of vocabulary). The value at $(x_i,w_j)$ indicates the number of words $w_j$ in the review $x_i$.\n",
        "\n",
        "For example: If the review $x_i$ is \"It is really really really good\", while the word $w_j$ is \"really\", then the value at $(i,j)$ is 3.\n",
        "\n",
        "However, there are too many words in the data, so the computation complexity will be large, while so words may be not that helpful for classification. Therefore, we will filter some words during vectorization according to the following criteria:\n",
        "\n",
        "- Filter those rare words that appear in **less than three reviews** among all reviews, which may be a typo or not English.\n",
        "- Filter those too common words appear in **more than 80% reviews**. (Like \"I\", \"you\", \"it\", these words are too common so that they can hardly help with classification)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZA5847Ohkz8",
        "outputId": "4a12e5c5-4e69-4db1-ca5c-4e53ca3b9d78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The shape of the bow matrix: (3279, 4597)\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    vectorizer = CountVectorizer(min_df=3, max_df=0.8)\n",
        "    #We filter those rare words that appear in less than three reviews among all reviews,\n",
        "    #      or those too common words appear in more than 80% reviews.\n",
        "    bow_matrix = vectorizer.fit_transform(full_review)\n",
        "\n",
        "    # bow_matrix is now a sparse matrix which is good for upcoming operation\n",
        "    print(f\"The shape of the bow matrix: {bow_matrix.shape}\") #shape (d, V)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k-0IQg5R1aq"
      },
      "source": [
        "Convert the pandas DataFrame into numpy array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrzoAlJHhpVC",
        "outputId": "e5047fa9-c592-464e-9f26-60e360d59da2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1 'dont waste your money' 'not worth a penny']\n",
            " [1 'limited run fail'\n",
            "  'the game itself is wonderful limited run ruined the release by pushing back multiple months do yourself a favor and buy the digital or buy from another company and stop giving limited run business']\n",
            " [4 'it coming'\n",
            "  'march  this character will be released at standard retail price in the us']\n",
            " ...\n",
            " [5 'simply amazing'\n",
            "  'this is a very deep dev kit that is very easy to pick up and well thought out navigation  using only the dualshock is good enough that move controllers are not necessary once you start playing around in dreams you really get addicted to exploring all the fantastic options while the tutorials are excellent and easy to comprehend  my only complaint is they should be a little better organized from the beginner to advanced lessons i think dreams is a huge milestone in the history of gaming thanks for making something so cool']\n",
            " [4 'i do love that the mic doesnt pick up a bunch of '\n",
            "  'headset works just as youd think speakers are full and without any crackle or fade while the mic pics up most speech  i do love that the mic doesnt pick up a bunch of background noise although at times it feels like i have to scream for it to pick up my voice  guess you cant have the best of both worlds but thatd be the loss of a star  if youre looking for a headset that will work for a very good price id recommend getting this one']\n",
            " [5 'best way to experience nintendos d on the switch'\n",
            "  'this headset is well made for the price decently comfortable  i had to adjust the straps so that they pull up just a little because i find that my big nose gathers condensation which can fog up the lenses  compared to that joke the nintendo labo d vr goggles these are a must buy for anyone who really wants to play switch in d  i am playing breath of the wild with these  it looks good but the pixels do kinda stand out too far  i dont know  for less than  bucks how can you go wrong']]\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    data = df_data.to_numpy()\n",
        "    print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_jHItqU7b3X"
      },
      "source": [
        "##### **Task0.2: Assign the sentiment labels**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv4edmAcGcFI"
      },
      "source": [
        "Assign the sentiment labels to the reviews according to the rating.\n",
        "\n",
        "* For reviews with rating of 4.0 or 5.0, assign \"1\" to these reviews as the \"positive\" label.\n",
        "\n",
        "* For reviews with rating of 1.0 or 2.0, assign \"0\" to these reviews as the \"negative\" label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFxo8DdmI8Dr",
        "outputId": "fff8a032-ff05-4238-fe28-94651d3e171f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1]\n"
          ]
        }
      ],
      "source": [
        "###Task0.2: Assign the sentiment labels###\n",
        "def label_assign(data):\n",
        "  '''\n",
        "  Split out the rating columns from the data array, and substitute the ratings with corresponding labels\n",
        "  IMPORTANT: You should not use any explicit loop in this function\n",
        "\n",
        "  Args: A 2D numpy array including ratings and augmented reviews\n",
        "  Returns: a 1D numpy array representing the labels according to the input rating\n",
        "  '''\n",
        "  ###TODO-2###\n",
        "\n",
        "\n",
        "  \n",
        "  ############\n",
        "  return assigned_labels\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  r = np.array([[2.0, 'bad', 'bad', 1], [4.0, 'good', 'good', 3]])\n",
        "  print(label_assign(r))\n",
        "  #expected output: [0 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnYUKmY6T9jt"
      },
      "source": [
        "##### Split out the training data and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8S2VJDWVns9"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    labels = label_assign(data)\n",
        "\n",
        "    ###Do not modify the code below\n",
        "    train_bow, test_bow, train_labels, test_labels  = train_test_split(bow_matrix, labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moWjd0qVI3jB"
      },
      "source": [
        "#### Part 1: Sentiment Categorization with Naïve Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-q1YY9A5kEO"
      },
      "source": [
        "The first goal is to classify the sentiment of the review. There are **N=2 predefined classes** (sentiment labels) which are \"1 (positive)\" and \"0 (negative)\" This is a classic text classification task. We will use a **Naïve Bayes classifier**.\n",
        "\n",
        "Let a training dataset be $(X,Y)$, where:\n",
        "- $X=\\{x_1, x_2, \\dots, x_d\\}$ is a set of $d$ reviews. Each review is represented as a vector of word counts.\n",
        "- $Y=\\{y_1, y_2, \\dots, y_d\\}$ are their corresponding class labels, where $y_i \\in \\{0, 1\\}$.\n",
        "\n",
        "**NOTICE:** For simplicity, we introduce a new concept called **Training Delta $\\delta_{ij}$**:  \n",
        "$\\delta_{ij}=1$ when $y_i=c_j$ and $\\delta_{ij}=0$ otherwise.\n",
        "\n",
        "To build our classifier, we need to estimate two probabilities from the training data:\n",
        "\n",
        "1.  **(Task1.1) Class Probabilities** $P(c_j)$ **(Task1.1)**: The prior probability of any given post belonging to class $c_j$.\n",
        "\n",
        "$$P(c_j) = \\frac{1 + \\text{count}(y=c_j)}{N + d} = \\frac{1 + \\sum_{i=1}^d \\delta_{ij}}{N + d}$$\n",
        "\n",
        "2.  **(Task1.2) Word Probabilities** $P(w_k|c_j)$ **(Task1.2)**: The conditional probability of observing word $w_k$ in a review, given that the review belongs to class $c_j$.\n",
        "\n",
        "We will use the following formulas with **Laplace (add-one) smoothing** to avoid zero probabilities for words that don't appear in the training data for a specific class.\n",
        "\n",
        "$$P(w_k|c_j) = \\frac{1 + \\text{count}(w_k \\text{ in class } c_j)}{|V| + \\sum_{s=1}^{|V|} \\text{count}(w_s \\text{ in class } c_j)} = \\frac{1 + \\sum_{i=1}^d \\delta_{ij} x_{ik}}{|V| + \\sum_{s=1}^{|V|} \\sum_{i=1}^d \\delta_{ij} x_{is}}$$\n",
        "\n",
        "Where:\n",
        "- $N$ is the number of classes.\n",
        "- $d$ is the number of training reviews.\n",
        "- $|V|$ is the size of the vocabulary (total number of unique words).\n",
        "- $x_{ik}$ is the number of times word $w_k$ appears in review $x_i$.\n",
        "- $\\delta_{ij}$ is an indicator: $\\delta_{ij}=1$ if review $x_i$ has class label $c_j$, and $0$ otherwise.\n",
        "\n",
        "**(Task1.3) Prediction**: To predict the sentiment class of a new review, we calculate the score for each class and choose the class with the highest score. For numerical stability, we work with **log probabilities**:\n",
        "\n",
        "$\\text{score}(c_j) = \\log(P(c_j)) + \\sum_{k=1}^{|V|} x_{ik} \\log(P(w_k|c_j))$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdNLe9co0-9M"
      },
      "outputs": [],
      "source": [
        "class NaiveBayesClassifier:\n",
        "  def __init__(self):\n",
        "    self.delta = None\n",
        "    self.class_probs = None\n",
        "    self.word_probs = None\n",
        "    self.vocabulary_size = None\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    \"\"\"\n",
        "    Trains the Naive Bayes classifier.\n",
        "\n",
        "    Args:\n",
        "      X: Training data (numpy array of word counts).\n",
        "      y: Training labels (numpy array).\n",
        "    \"\"\"\n",
        "    self.class_probs = self.compute_class_prob(X, y)\n",
        "    self.word_probs = self.compute_word_prob(X, y)\n",
        "\n",
        "  ###Task1.1: Compute the class probabilities###\n",
        "  def compute_class_prob(self, X, y):\n",
        "    \"\"\"\n",
        "    Computes the class probabilities matrix P(c_j) following the given formulas.\n",
        "\n",
        "    Args:\n",
        "      X: Training data (numpy array of word counts).\n",
        "      y: Training labels (numpy array).\n",
        "\n",
        "    Returns:\n",
        "      Class probabilities matrix: a 1D numpy array with shape (2,).\n",
        "    \"\"\"\n",
        "    ###TODO-3###\n",
        "    \n",
        "    \n",
        "    \n",
        "    ############\n",
        "    return class_probabilities\n",
        "\n",
        "  ###Task1.2: Compute the word probabilities###\n",
        "  def compute_word_prob(self, X, y):\n",
        "    \"\"\"\n",
        "    Computes the word probabilities matrix P(w_k|c_j) following the given formulas.\n",
        "\n",
        "    Args:\n",
        "      X: Training data: numpy array of word counts with shape: (d, |V|).\n",
        "      y: Training labels (numpy array).\n",
        "      delta: Training Delta matrix\n",
        "\n",
        "    Returns:\n",
        "      Word probabilities matrix: a 2D numpy array with shape (vocabulary_size, 2).\n",
        "    \"\"\"\n",
        "    ###TODO-4###\n",
        "    \n",
        "    \n",
        "    \n",
        "    ############\n",
        "    return word_probabilities\n",
        "\n",
        "  ###Task1.3: Predicts the class labels for new data###\n",
        "  def predict(self, X):\n",
        "    \"\"\"\n",
        "    Compute the scores for each class and each review to predict the class label with the highest score.\n",
        "\n",
        "    Args:\n",
        "      X: Test data (sparse matrix or numpy array of word counts).\n",
        "\n",
        "    Returns:\n",
        "      Predicted labels (numpy array).\n",
        "\n",
        "    NOTICE: You should compute the log number of class_probs and word_probs according to the given formula to ensure numerical stability.\n",
        "    \"\"\"\n",
        "    ###TODO-5###\n",
        "    \n",
        "    \n",
        "    \n",
        "    ############\n",
        "    return predicted_labels\n",
        "\n",
        "\n",
        "  def evaluate(self, X, y):\n",
        "    \"\"\"\n",
        "    Evaluates the classifier accuracy.\n",
        "\n",
        "    Args:\n",
        "      X: Test data (sparse matrix or numpy array of word counts).\n",
        "      y: True labels (numpy array).\n",
        "\n",
        "    Returns:\n",
        "      Accuracy (float).\n",
        "    \"\"\"\n",
        "    predicted_labels = self.predict(X)\n",
        "    accuracy = np.mean(predicted_labels == y)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pJTqAAOpANk"
      },
      "source": [
        "##### Evaluate the model with the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8SRe0tmu6LO"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  nb_classifier = NaiveBayesClassifier()\n",
        "  nb_classifier.fit(train_bow, train_labels)\n",
        "  accuracy = nb_classifier.evaluate(test_bow, test_labels)\n",
        "  print(f\"Accuracy: {accuracy}\")\n",
        "  # Expected output: Accuracy: 0.8932926829268293"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdAIaTPip7FO"
      },
      "source": [
        "#### Part 2: Word Importance Analysis with Naïve Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdBRWM9sEImd"
      },
      "source": [
        "\n",
        "In this part, we will dive deeper into the Naïve Bayes Classifier by not just classifying sentiment, but also **interpreting which words are most influential for each class (positive or negative reviews)**. This is a crucial skill in real-world machine learning: understanding *why* your model makes its predictions, and which features (words) is dominant in its decisions.\n",
        "\n",
        "##### Task Overview\n",
        "\n",
        "1. (Task2.1) **Calculate log odds ratios** to **compare its likelihood in positive vs. negative reviews**.\n",
        "\n",
        "2. (Task2.2) **Identify the top \"indicator\" words** for both positive and negative sentiments given the calculated log odds ratios in Task2.1.\n",
        "3. (Task2.3) **Identify the most confusing/neutral words**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIPPoJNCXjy-"
      },
      "source": [
        "##### **Task 2.1**: Compute Log Odds Ratios\n",
        "\n",
        "Given the conditional probabilities $P(w_k|c_j)$ from your trained model, you can compute the **log odds ratio** for each word $w_k$ as:\n",
        "$$\n",
        "\\text{log-odds}(w_k) = \\log \\left( \\frac{P(w_k|c_1)}{P(w_k|c_0)} \\right)\n",
        "$$\n",
        "- If $\\text{log-odds}(w_k)$ is **high and positive**, $w_k$ is strongly associated with positive reviews.\n",
        "- If $\\text{log-odds}(w_k)$ is **low and negative**, $w_k$ is strongly associated with negative reviews.\n",
        "\n",
        "This analysis helps you see *which words your model is \"paying attention to\"* for each sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4X92Aa2C0bC"
      },
      "outputs": [],
      "source": [
        "###Task2.1: Compute Log Odds Ratios Matrix###\n",
        "def compute_log_odds(word_probs):\n",
        "    \"\"\"\n",
        "    Compute the log odds ratio for each word in the vocabulary.\n",
        "\n",
        "    Args:\n",
        "        word_probs: numpy array of shape (vocab_size, 2), where [:,0] is P(w|c=0), [:,1] is P(w|c=1)\n",
        "\n",
        "    Returns:\n",
        "        log_odds: numpy array of shape (vocab_size,), log-odds for each word\n",
        "    \"\"\"\n",
        "    ###TODO-6###\n",
        "    \n",
        "    \n",
        "    \n",
        "    ############\n",
        "    return log_odds\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_numpy_array = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n",
        "    log_odds = compute_log_odds(test_numpy_array)\n",
        "    print(log_odds)\n",
        "    #Expected Output: [0.69314718 0.28768207 0.18232156]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXMFUTLT8aL4"
      },
      "source": [
        "##### **Task2.2**: Identify Top Positive/Negative Indicator Words\n",
        "\n",
        "Given the calculated log odds ratios in Task2.1, we can compute the **top k positive/negative** words from our review data. In other words, in this task, we're going to identify which words are **most impactful** when classifying a review's sentiment.\n",
        "\n",
        "You can rank the odds ratios of those words, and then find the top k words with largest/smallest value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9_-wO4h8m8R"
      },
      "outputs": [],
      "source": [
        "###Task2.2: Identify Top Positive/Negative Indicator Words\n",
        "def find_top_words(log_odds, vocab, top_k=10, indicator=True):\n",
        "    \"\"\"\n",
        "    Find the top_k words with the highest/lowest log odds ratio.\n",
        "    You may need to us the function numpy.argsort() to sort the log_odds array.\n",
        "    You can refer to this page for your answer: https://numpy.org/doc/stable/reference/generated/numpy.argsort.html\n",
        "\n",
        "    Args:\n",
        "        log_odds: numpy array of shape (vocab_size,), log-odds for each word\n",
        "        vocab: numpy array of shape (vocab_size,), vocabulary, which maps indices to words\n",
        "        top_k: int, number of top words to return\n",
        "        indicator: bool, True for positive indicators, False for negative indicators\n",
        "\n",
        "    Returns:\n",
        "        top_words: list of word\n",
        "        top_scores: list of log-odds scores\n",
        "        idx: list of indices of the top words in the vocabulary\n",
        "    \"\"\"\n",
        "    ###TODO-7###\n",
        "\n",
        "\n",
        "\n",
        "    ############\n",
        "    return top_words, top_scores, idx\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    log_odds = compute_log_odds(nb_classifier.word_probs)\n",
        "    vocab = np.array(vectorizer.get_feature_names_out())\n",
        "    print(\"Top 10 Positive Indicator Words:\")\n",
        "    top_pos_words, top_pos_scores, top_pos_idx = find_top_words(log_odds, vocab, top_k=10, indicator=True)\n",
        "    for w, s in zip(top_pos_words, top_pos_scores):\n",
        "        print(f\"{w:15s}  {s:.3f}\")\n",
        "    ##############################\n",
        "    #  Expected Output:\n",
        "    ##############################\n",
        "    #  Top 10 Positive Indicator Words:\n",
        "    #  surprisingly     2.796\n",
        "    #  fighter          2.796\n",
        "    #  de               2.628\n",
        "    #  amazing          2.582\n",
        "    #  inexpensive      2.582\n",
        "    #  wired            2.582\n",
        "    #  grandson         2.582\n",
        "    #  adults           2.582\n",
        "    #  detail           2.533\n",
        "    #  advance          2.482\n",
        "    ##############################\n",
        "    print(\"\\nTop 10 Negative Indicator Words:\")\n",
        "    top_neg_words, top_neg_scores, top_neg_idx = find_top_words(log_odds, vocab, top_k=10, indicator=False)\n",
        "    for w, s in zip(top_neg_words, top_neg_scores):\n",
        "        print(f\"{w:15s}  {s:.3f}\")\n",
        "    ##############################\n",
        "    # Expected Output:\n",
        "    ##############################\n",
        "    #  Top 10 Negative Indicator Words:\n",
        "    #  horrible         -4.004\n",
        "    #  awful            -3.864\n",
        "    #  junk             -3.554\n",
        "    #  false            -3.507\n",
        "    #  unplayable       -3.407\n",
        "    #  refund           -3.296\n",
        "    #  poorly           -3.235\n",
        "    #  unacceptable     -3.235\n",
        "    #  gouging          -3.171\n",
        "    #  trash            -3.137\n",
        "    ##############################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUnXM9fKYqOO"
      },
      "outputs": [],
      "source": [
        "### Visualize Results\n",
        "if __name__ == \"__main__\":\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.bar(top_pos_words, top_pos_scores, color='skyblue')\n",
        "    plt.title(\"Top 10 Positive Indicator Words (Log Odds)\")\n",
        "    plt.ylabel(\"Log Odds\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.bar(top_neg_words, top_neg_scores, color='gray')\n",
        "    plt.title(\"Top 10 Negative Indicator Words (Log Odds)\")\n",
        "    plt.ylabel(\"Log Odds\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUZ3lgC5ffPr"
      },
      "source": [
        "##### **Task 2.3**: Identify Most Confusing Words (\"Ambiguous Words\")\n",
        "Some words may appear frequently in both positive and negative reviews, making them poor discriminators of sentiment. These are \"ambiguous words.\"\n",
        "\n",
        "In this task, find the top k words whose log odds ratios are closest to zero (i.e., most ambiguous)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPiRdOv7HZkk"
      },
      "outputs": [],
      "source": [
        "### Task 2.3: Identify Most Confusing Words (\"Ambiguous Words\")\n",
        "def find_ambiguous_words(log_odds, vocab, top_k=10):\n",
        "    \"\"\"\n",
        "    Find the top_n words whose log odds ratio is closest to zero.\n",
        "    You may need to us the function numpy.argsort().\n",
        "\n",
        "    Args:\n",
        "        log_odds: numpy array of shape (vocab_size,), log-odds for each word\n",
        "        vocab: numpy array of shape (vocab_size,), vocabulary, which maps indices to words\n",
        "        top_k: int, number of top ambiguous words to return\n",
        "\n",
        "    Returns:\n",
        "        ambiguous_words: list of word strings\n",
        "        ambiguous_scores: list of log-odds scores\n",
        "    \"\"\"\n",
        "    ###TODO-8###\n",
        "    \n",
        "    \n",
        "    \n",
        "    ############\n",
        "    return ambiguous_words, ambiguous_scores\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ambiguous_words, ambiguous_scores = find_ambiguous_words(log_odds, vocab)\n",
        "    print(\"Most Ambiguous Words (Log Odds ~ 0):\")\n",
        "    for w, s in zip(ambiguous_words, ambiguous_scores):\n",
        "        print(f\"{w:15s}  {s:.3f}\")\n",
        "    ###############################\n",
        "    #  Expected Output:\n",
        "    ###############################\n",
        "    #  Most Ambiguous Words (Log Odds ~ 0):\n",
        "    #  really           0.002\n",
        "    #  if               0.003\n",
        "    #  mean             -0.003\n",
        "    #  seen             -0.006\n",
        "    #  directions       0.007\n",
        "    #  prices           0.007\n",
        "    #  speed            0.007\n",
        "    #  tracks           0.007\n",
        "    #  generation       0.007\n",
        "    #  building         0.007\n",
        "    ###############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W0jnZGBjvCL"
      },
      "source": [
        "#### Part 3: Model Evaluation with Confusion Matrix\n",
        "\n",
        "In real-world applications, accuracy alone isn't enough to evaluate a model's performance. A confusion matrix provides a more detailed breakdown of correct and incorrect predictions, allowing us to understand the types of errors our model makes.\n",
        "\n",
        "##### Task Overview\n",
        "\n",
        "1. (Task3.1) **Compute the confusion matrix** for your Naive Bayes classifier.\n",
        "2. (Task3.2) **Calculate precision, recall, and F1-score** from the confusion matrix.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Snsu1C_PkPGV"
      },
      "source": [
        "##### **Task 3.1**: Compute Confusion Matrix\n",
        "\n",
        "Given the ground truth labels (y_true) and your model's predicted labels (y_pred), we build a (2, 2) shaped numpy array that summarizes the results of the classification including the following four values.\n",
        "\n",
        "- **True Negatives (TN):** The number of observations correctly identified as negative.\n",
        "\n",
        "- **False Positives (FP):** The number of observations incorrectly identified as positive.\n",
        "\n",
        "- **False Negatives (FN):** The number of observations incorrectly identified as negative.\n",
        "\n",
        "- **True Positives (TP):** The number of observations correctly identified as positive.\n",
        "\n",
        "Your code will need to iterate through the lists of true and predicted labels and, for each pair, increment the corresponding counter in the confusion matrix. Remember that the matrix is structured as follows:\n",
        "\n",
        "| | Predicted Negative (0) | Predicted Positive (1) |\n",
        "|---|---|---|\n",
        "| **Actual Negative (0)** | TN (`[0,0]`) | FP (`[0,1]`) |\n",
        "| **Actual Positive (1)** | FN (`[1,0]`) | TP (`[1,1]`) |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mnoaekgjtST"
      },
      "outputs": [],
      "source": [
        "###Task3.1: Compute Confusion Matrix###\n",
        "def compute_confusion_matrix(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute the confusion matrix for binary classification.\n",
        "\n",
        "    Args:\n",
        "        y_true: numpy array of shape (n_samples,), ground truth labels\n",
        "        y_pred: numpy array of shape (n_samples,), predicted labels\n",
        "\n",
        "    Returns:\n",
        "        confusion_matrix: numpy array of shape (2, 2), where:\n",
        "            [0,0] = True Negatives (TN)\n",
        "            [0,1] = False Positives (FP)\n",
        "            [1,0] = False Negatives (FN)\n",
        "            [1,1] = True Positives (TP)\n",
        "    \"\"\"\n",
        "    ###TODO-9###\n",
        "    \n",
        "    \n",
        "    \n",
        "    ############\n",
        "    return confusion_matrix\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Get predictions on test data\n",
        "    test_predictions = nb_classifier.predict(test_bow)\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    conf_matrix = compute_confusion_matrix(test_labels, test_predictions)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "    # Expected Output: Confusion Matrix:\n",
        "    #                    [[237  32]\n",
        "    #                    [ 38 349]]\n",
        "\n",
        "    # Visualize confusion matrix\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "                yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTbeWFp_kTN1"
      },
      "source": [
        "##### **Task 3.2**: Calculate Precision, Recall, and F1-Score\n",
        "\n",
        "Given the confusion matrix, calculate the following metrics:\n",
        "\n",
        "1. **Precision**: The ratio of correctly predicted positive observations to the total predicted positives.\n",
        "   - $Precision = \\frac{TP}{(TP + FP)}$\n",
        "\n",
        "2. **Recall**: The ratio of correctly predicted positive observations to all actual positives. That is to calculate, of all the instances that were actually positive, what proportion did the model **correctly identify**?\n",
        "   - $Recall = \\frac{TP}{(TP + FN)}$\n",
        "\n",
        "3. **F1-Score**: The harmonic mean of precision and recall. This is a balanced measure that considers both false positives and false negatives, which is particularly useful when you have an uneven class distribution.\n",
        "   - $F1 = 2 \\times \\frac{(Precision \\times Recall)}{(Precision + Recall)}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhZQtDpzkUaL"
      },
      "outputs": [],
      "source": [
        "###Task3.2: Calculate Precision, Recall, and F1-Score###\n",
        "def calculate_metrics(confusion_matrix):\n",
        "    \"\"\"\n",
        "    Calculate precision, recall, and F1-score from a confusion matrix.\n",
        "    You may use the previous functions that we implement\n",
        "\n",
        "    Args:\n",
        "        confusion_matrix: numpy array of shape (2, 2)\n",
        "\n",
        "    Returns:\n",
        "        three values: precision, recall, and f1_score\n",
        "    \"\"\"\n",
        "    ###TODO-10###\n",
        "    \n",
        "    \n",
        "    \n",
        "    ############\n",
        "    return precision, recall, f1_score\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Get predictions on test data\n",
        "    test_predictions = nb_classifier.predict(test_bow)\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    conf_matrix = compute_confusion_matrix(test_labels, test_predictions)\n",
        "    # Calculate metrics\n",
        "    metrics = calculate_metrics(conf_matrix)\n",
        "    print(\"\\nModel Performance Metrics:\")\n",
        "    print(f\"Precision: {metrics[0]:.4f}\")\n",
        "    print(f\"Recall: {metrics[1]:.4f}\")\n",
        "    print(f\"F1-Score: {metrics[2]:.4f}\")\n",
        "    # Expected Output:\n",
        "    # Model Performance Metrics:\n",
        "    #    Precision: 0.9160\n",
        "    #    Recall: 0.9018\n",
        "    #    F1-Score: 0.9089"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Remember to put any debugging code under: \n",
        "```\n",
        "if __name__ == \"__main__\": \n",
        "```\n",
        "##### to avoid any erros while testing on ZINC!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
